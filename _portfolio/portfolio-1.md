---
title: "Maximal Matching for Cross-Modal Retrieval"
excerpt: "Diversity-aware image-text retrieval using maximal matching objectives and multi-prompt embeddings to prevent representation collapse<br/><img src='/images/profile.png'>"
collection: portfolio
---

## Project Overview
Led research on diversity-aware cross-modal retrieval systems that prevent representation collapse through maximal matching objectives and multi-prompt embedding strategies. This work resulted in a publication at **ACL 2025 (Main Conference)**.

## Key Contributions
- Developed novel maximal matching objectives that preserve semantic diversity in cross-modal embeddings
- Designed multi-prompt embedding strategy that captures literal, figurative, emotional, and cultural aspects of meaning
- Achieved **7.1% performance improvement** over baseline methods on standard benchmarks
- Built evaluation framework to measure representation collapse and semantic diversity

## Technical Approach
- **Architecture:** Two-stage retrieval pipeline with lightweight retrievers and cross-modal rerankers
- **Models:** Vision-Language Models (VLMs), CLIP-based encoders, transformer-based rerankers
- **Datasets:** Extended COCO, Flickr30k with diverse semantic annotations
- **Framework:** PyTorch, Hugging Face Transformers, Weights & Biases

## Impact
This research addresses fundamental limitations in current retrieval systems by ensuring they capture semantic diversity rather than collapsing to a single dominant interpretation. Applications include search engines, content recommendation, and accessibility tools.

## Publication
**Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval**
*ACL 2025 (Main Conference)*, Vienna, Austria

[Google Scholar](https://scholar.google.com/citations?user=Ft_qTcwAAAAJ&hl=en) | [ResearchGate](https://www.researchgate.net/profile/Hani-Al-Omari-2)
