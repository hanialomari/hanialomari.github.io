---
title: "ENTER & JourneyBench: Interpretable Video Reasoning"
excerpt: "Event-based interpretable reasoning for VideoQA and long-form video understanding benchmarks (NeurIPS 2024)<br/><img src='/images/profile.png'>"
collection: portfolio
---

## Project Overview
Collaborated with Columbia University on two major video understanding projects: **ENTER** (Event-Based Interpretable Reasoning for VideoQA) and **JourneyBench** (long-form video understanding benchmark). Both works were published at **NeurIPS 2024**, with ENTER also appearing at the MAR Workshop.

## ENTER: Event-Based Interpretable Reasoning

### Key Contributions
- Developed event-based reasoning framework for interpretable VideoQA systems
- Designed structured knowledge extraction from video content
- Created evaluation metrics for reasoning interpretability
- Contributed to modeling architecture, annotation pipeline, and evaluation framework

### Technical Approach
- **Architecture:** Event extraction networks with temporal reasoning modules
- **Interpretability:** Structured event representations with causal chains
- **Training:** Multi-task learning combining QA and event detection
- **Evaluation:** Novel metrics measuring reasoning quality and interpretability

## JourneyBench: Long-Form Video Understanding

### Key Contributions
- Built benchmark for evaluating models on long-form video understanding
- Designed annotation protocols for complex temporal relationships
- Contributed to dataset construction and evaluation methodology
- Analyzed model performance across different video lengths and reasoning types

### Technical Approach
- **Dataset:** Long-form videos with hierarchical question structure
- **Annotation:** Multi-level temporal and semantic annotations
- **Evaluation:** Comprehensive benchmark covering various reasoning capabilities
- **Analysis:** Performance breakdown by video length, question type, reasoning depth

## Impact
These projects advance video understanding research by focusing on interpretable reasoning and long-form temporal relationships. ENTER provides frameworks for building explainable VideoQA systems, while JourneyBench enables systematic evaluation of long-form video understanding capabilities.

## Publications
- **ENTER:** Event-Based Interpretable Reasoning for VideoQA
  *NeurIPS 2024 & MAR Workshop @ NeurIPS 2024*

- **JourneyBench:** Long-Form Video Understanding Benchmark
  *NeurIPS 2024*

## Collaboration
Multi-institution collaboration with Columbia University, demonstrating ability to work in distributed research teams and contribute to large-scale projects.

## Skills Demonstrated
- Video understanding and temporal reasoning
- Multi-modal learning (vision + language)
- Interpretable AI and explainability
- Large-scale dataset construction
- Benchmark design and evaluation methodology
- Cross-institutional research collaboration

[Google Scholar](https://scholar.google.com/citations?user=Ft_qTcwAAAAJ&hl=en) | [ResearchGate](https://www.researchgate.net/profile/Hani-Al-Omari-2)
