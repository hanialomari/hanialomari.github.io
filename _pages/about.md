---
permalink: /
title: "Hi, I'm Hani"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<div class="hero-banner">
  <div class="hero-content">
    <p class="eyebrow">Multimodal Retrieval • VLMs • Diversity-Aware Search</p>
    <h1>Building retrieval that understands images, text, video, and sound.</h1>
    <p class="lead">I'm a Ph.D. researcher at Virginia Tech's Sanghani Center working with <a href="https://people.cs.vt.edu/chris/">Dr. Chris Thomas</a>. I design retrieval and RAG systems that surface meaningful, unexpected connections across modalities.</p>
    <div class="hero-actions">
      <a class="button button--primary" href="mailto:hanialomari@vt.edu">Let's collaborate</a>
      <a class="button button--ghost" href="/publications/">View publications</a>
    </div>
    <div class="hero-chips">
      <span>Multi-prompt embeddings</span>
      <span>Cross-modal reranking</span>
      <span>Diversity-aware evaluation</span>
      <span>RAG with VLM readers</span>
    </div>
  </div>
  <div class="hero-card">
    <div class="hero-glow"></div>
    <div class="hero-card__inner">
      <p class="hero-label">Now</p>
      <ul>
        <li>Training a VLM-based retriever with a cross-modal reranker.</li>
        <li>Building a RAG workflow that treats the VLM as a "reader" to keep grounding.</li>
        <li>Exploring room-impulse responses (RIRs) for spatial cues in generative scene understanding.</li>
      </ul>
    </div>
  </div>
</div>

<div class="section">
  <div class="section-header">
    <p class="eyebrow">Focus areas</p>
    <h2>Designing resilient multimodal systems</h2>
    <p>My work blends robust retrieval objectives, interpretable rerankers, and evaluation pipelines that celebrate nuance over a single "best" vector.</p>
  </div>
  <div class="pill-grid">
    <div class="pill-card">
      <h3>Multi-prompt embeddings</h3>
      <p>Generating many small views of meaning to capture literal, figurative, emotional, abstract, and cultural cues.</p>
    </div>
    <div class="pill-card">
      <h3>Two-stage retrieval</h3>
      <p>Lightweight retrievers paired with cross-modal rerankers and VLM "readers" for grounded reasoning.</p>
    </div>
    <div class="pill-card">
      <h3>Diversity-aware evaluation</h3>
      <p>Metrics and datasets that reward semantic breadth and avoid representation collapse.</p>
    </div>
  </div>
</div>

<div class="section">
  <div class="section-header">
    <p class="eyebrow">Latest news</p>
    <h2>Updates from the lab</h2>
  </div>
  <div class="news-timeline">
    <div class="news-item">
      <span class="news-date">Feb 2025</span>
      <div>
        <p class="news-title">Preparing camera-ready material for ACL 2025 (Main) — Maximal Matching Matters.</p>
        <p class="news-detail">Adding new qualitative analyses on how diversity-aware prompts prevent cross-modal collapse.</p>
      </div>
    </div>
    <div class="news-item">
      <span class="news-date">Dec 2024</span>
      <div>
        <p class="news-title">NeurIPS 2024 & MAR spotlight for JourneyBench + ENTER.</p>
        <p class="news-detail">Showcased interpretable video reasoning with retrieval-grounded event traces.</p>
      </div>
    </div>
    <div class="news-item">
      <span class="news-date">Oct 2024</span>
      <div>
        <p class="news-title">Under review: unified multimodal retriever with spatial audio cues.</p>
        <p class="news-detail">New experiments on room-impulse responses to enrich generative scene understanding.</p>
      </div>
    </div>
  </div>
</div>

<div class="section">
  <div class="section-header">
    <p class="eyebrow">Featured publications</p>
    <h2>Latest papers</h2>
    <p>Full list lives on the <a href="/publications/">publications page</a>. Here are the three freshest projects.</p>
  </div>
  <div class="publication-grid">
    <article class="paper-card">
      <div class="paper-thumb">Cover coming soon</div>
      <div class="paper-meta">
        <p class="paper-venue">ACL 2025 (Main)</p>
        <h3>Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval.</h3>
        <p>Multi-prompt embeddings and diversity-aware reranking that keep retrieval balanced across modalities.</p>
      </div>
    </article>
    <article class="paper-card">
      <div class="paper-thumb">Placeholder</div>
      <div class="paper-meta">
        <p class="paper-venue">NeurIPS 2024 & MAR @ NeurIPS</p>
        <h3>JourneyBench & ENTER: Event-Based Interpretable Reasoning for VideoQA.</h3>
        <p>Benchmark and models that narrate video reasoning with transparent event traces.</p>
      </div>
    </article>
    <article class="paper-card">
      <div class="paper-thumb">Under review</div>
      <div class="paper-meta">
        <p class="paper-venue">Under Review (2025)</p>
        <h3>Unified Multimodal Retriever with Spatial Audio Cues.</h3>
        <p>Room-impulse-informed retrieval that grounds generative scene understanding with spatial awareness.</p>
      </div>
    </article>
  </div>
</div>

<div class="section section--split">
  <div>
    <p class="eyebrow">Collaborations</p>
    <h2>Let's build meaningful retrieval</h2>
    <p>I'm excited about research partnerships, internships, and consulting on multimodal retrieval, VLM interpretability, and evaluation. If you're building diversity-aware systems or new datasets, let's chat.</p>
    <div class="hero-actions">
      <a class="button button--primary" href="mailto:hanialomari@vt.edu">Email me</a>
      <a class="button button--ghost" href="https://www.linkedin.com/in/hanialomari/">Connect on LinkedIn</a>
    </div>
  </div>
  <div class="stat-card">
    <p class="stat-eyebrow">Impact snapshots</p>
    <ul>
      <li><strong>3x</strong> top-tier venues in the last year (ACL, NeurIPS, FGVC).</li>
      <li><strong>Multi-modal</strong> pipelines spanning images, text, video, and sound.</li>
      <li><strong>Diversity-first</strong> retrieval objectives and metrics.</li>
    </ul>
  </div>
</div>
