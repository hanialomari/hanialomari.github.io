---
permalink: /
title: "Hi, I'm Hani"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

## About Me

I'm a Ph.D. researcher at Virginia Tech's Sanghani Center for Artificial Intelligence and Data Analytics, specializing in **multimodal learning** and **vision-language models**. I work with [Dr. Chris Thomas](https://people.cs.vt.edu/chris/) to build AI systems that understand **images, text, video, and sound** -- not just literal matches, but the meaningful connections across modalities.

My research combines cutting-edge techniques in **computer vision**, **natural language processing**, and **deep learning** to create retrieval systems that capture semantic diversity, cultural nuance, and abstract relationships.

---

## Open to Opportunities

I'm actively seeking **research internships** and collaboration opportunities in:
- Multimodal AI and Vision-Language Models
- Cross-Modal Retrieval Systems
- Natural Language Processing
- Computer Vision and Deep Learning
- Retrieval-Augmented Generation (RAG)

**Available:** Summer 2025 and beyond
**Contact:** [hani@vt.edu](mailto:hani@vt.edu) | [LinkedIn](https://linkedin.com/in/hanialomari)

---

## Research Focus

### Core Areas
- **Multi-prompt embeddings** that generate diverse semantic "views" to prevent representation collapse and improve retrieval fairness
- **Two-stage retrieval pipelines** combining lightweight retrievers with cross-modal rerankers and VLM "readers" for grounded reasoning
- **Diversity-aware retrieval** that captures literal, figurative, emotional, abstract, and cultural cues beyond single-vector representations

### Why It Matters
Modern AI systems must bridge visual, textual, and auditory understanding to support creativity, safety, and accessibility. My research develops new objectives, evaluation frameworks, and datasets to ensure retrieval systems work for diverse users and use cases -- from idioms and metaphors to cultural nuances and underrepresented perspectives.

---

## Current Projects

- Training VLM-based retrievers with cross-modal rerankers and multi-prompt embeddings for diversity-guided search
- Building RAG workflows that treat VLMs as "readers" while maintaining factual grounding
- Exploring room-impulse responses (RIRs) for spatial audio cues in generative scene understanding
- Collaborating on interpretable VideoQA and visual reasoning benchmarks

---

## Recent Publications

- **ACL 2025 (Main)** -- [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://scholar.google.com/citations?user=Ft_qTcwAAAAJ&hl=en)
- **NeurIPS 2024** & **MAR @ NeurIPS 2024** -- JourneyBench and ENTER (Event-Based Interpretable Reasoning for VideoQA)
- **FGVC @ CVPR 2025** -- Real-Time Ultra-Fine-Grained Surgical Instrument Classification

[View all publications](/publications/)

---

## Technical Skills

**Languages:** Python, C++, Java, SQL, Bash, LaTeX
**ML/AI:** PyTorch, TensorFlow, Transformers, scikit-learn, Keras
**Tools:** Git, Docker, Weights & Biases, Conda
**Specialties:** Vision-Language Models, Multimodal Retrieval, NLP, Computer Vision, Deep Learning

---

## Let's Connect

Whether you're working on diversity-aware retrieval, interpretable VLMs, or building new evaluation benchmarks, I'd love to discuss potential collaborations or opportunities.

**Email:** [hani@vt.edu](mailto:hani@vt.edu)
**LinkedIn:** [linkedin.com/in/hanialomari](https://linkedin.com/in/hanialomari)
**GitHub:** [github.com/hanialomari](https://github.com/hanialomari)
**Google Scholar:** [View Publications](https://scholar.google.com/citations?user=Ft_qTcwAAAAJ&hl=en)