---
permalink: /
title: "Hi, I'm Hani"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---
<div class="hero-section">
  <div class="hero-copy">
    <p class="eyebrow">Multimodal Retrieval â€¢ Vision-Language â€¢ Responsible AI</p>
    <h1>Hi, Iâ€™m Hani Almari.</h1>
    <p class="lede">
      I build retrieval that understands <strong>images, text, video, and sound</strong> &mdash; not just the literal match.
      Iâ€™m a Ph.D. researcher at Virginia Techâ€™s Sanghani Center working on multimodal learning with
      <a href="https://people.cs.vt.edu/chris/">Dr. Chris Thomas</a>. My work combines vision-language models,
      retrieval-augmented generation, and ranking systems to surface meaningful, unexpected connections across modalities.
    </p>
    <div class="hero-actions">
      <a class="button primary" href="/cv/">View CV</a>
      <a class="button ghost" href="/publications/">Browse Publications</a>
    </div>
    <div class="hero-tags">
      <span>Multi-prompt embeddings</span>
      <span>Diversity-aware retrieval</span>
      <span>Cross-modal reranking</span>
      <span>Interpretability</span>
    </div>
  </div>
  <div class="hero-panel">
    <div class="stat-card">
      <p class="stat-label">Current Focus</p>
      <h3>Trustworthy multimodal retrieval</h3>
      <ul>
        <li>Training VLM-based retrievers with cross-modal rerankers.</li>
        <li>RAG workflows that keep factual grounding.</li>
        <li>Room-impulse responses for spatial cues in generative scene understanding.</li>
      </ul>
    </div>
    <div class="stat-row">
      <div>
        <p class="stat-value">3</p>
        <p class="stat-label">Latest Papers</p>
      </div>
      <div>
        <p class="stat-value">+5</p>
        <p class="stat-label">Active Projects</p>
      </div>
      <div>
        <p class="stat-value">âˆ</p>
        <p class="stat-label">Curiosity</p>
      </div>
    </div>
  </div>
</div>

<section class="highlight-grid">
  <div class="section-header">
    <h2>What Iâ€™m building</h2>
    <p>Techniques that keep retrieval diverse, interpretable, and grounded.</p>
  </div>
  <div class="grid">
    <div class="card">
      <p class="card-emoji">ğŸ§­</p>
      <h3>Multi-prompt embeddings</h3>
      <p>Generating many small â€œviewsâ€ of meaning so search stays rich and resilient against collapse.</p>
    </div>
    <div class="card">
      <p class="card-emoji">ğŸ›°ï¸</p>
      <h3>Two-stage pipelines</h3>
      <p>Lightweight retrievers paired with cross-modal rerankers and VLM readers for grounded reasoning.</p>
    </div>
    <div class="card">
      <p class="card-emoji">ğŸŒˆ</p>
      <h3>Diversity-aware retrieval</h3>
      <p>Capturing literal, figurative, emotional, and cultural cues rather than a single one-size vector.</p>
    </div>
    <div class="card">
      <p class="card-emoji">ğŸ”</p>
      <h3>Interpretable VLMs</h3>
      <p>Evaluation pipelines, objectives, and datasets that keep models honest, transparent, and fair.</p>
    </div>
  </div>
</section>

<section class="news-section">
  <div class="section-header">
    <h2>News &amp; updates</h2>
    <p>Highlights from the lab and the field.</p>
  </div>
  <div class="news-grid">
    <article class="news-item">
      <p class="news-date">Jan 2025</p>
      <h3>ACL 2025 paper accepted to the main conference</h3>
      <p>â€œMaximal Matching Mattersâ€ dives into preventing representation collapse for robust cross-modal retrieval.</p>
    </article>
    <article class="news-item">
      <p class="news-date">Dec 2024</p>
      <h3>NeurIPS spotlight on JourneyBench + ENTER</h3>
      <p>Showcasing interpretable event-based reasoning for video QA and safer VLM decision-making.</p>
    </article>
    <article class="news-item">
      <p class="news-date">Oct 2024</p>
      <h3>New preprint under review</h3>
      <p>Exploring spatial acoustics for generative scene understandingâ€”stay tuned for dataset release.</p>
    </article>
  </div>
</section>

<section class="pub-section">
  <div class="section-header">
    <h2>Latest publications</h2>
    <p>Fresh off the pressâ€”three snapshots of what Iâ€™m excited about.</p>
  </div>
  <div class="pub-grid">
    <div class="pub-card">
      <div class="pub-thumb placeholder">Paper visual coming soon</div>
      <div class="pub-meta">
        <span class="badge">ACL 2025 (Main)</span>
        <h3>Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval</h3>
        <p>Robust multi-modal retrieval with diversity-aware embeddings and collapse-resistant training.</p>
      </div>
    </div>
    <div class="pub-card">
      <div class="pub-thumb placeholder">Paper visual coming soon</div>
      <div class="pub-meta">
        <span class="badge">NeurIPS 2024</span>
        <h3>JourneyBench &amp; ENTER: Event-Based Interpretable Reasoning for VideoQA</h3>
        <p>Interpretable VLM pipelines that answer temporal questions with transparent evidence trails.</p>
      </div>
    </div>
    <div class="pub-card">
      <div class="pub-thumb placeholder">Paper visual coming soon</div>
      <div class="pub-meta">
        <span class="badge">Under review</span>
        <h3>Spatial Acoustics for Generative Scene Understanding</h3>
        <p>Leveraging room-impulse responses to enrich generative models with spatial cues and realism.</p>
      </div>
    </div>
  </div>
</section>

<section class="callout">
  <div>
    <h2>Letâ€™s build diverse, trustworthy retrieval together.</h2>
    <p>If youâ€™re building diversity-aware retrieval, interpretable VLMs, or new evaluation benchmarks, Iâ€™d love to collaborate.</p>
  </div>
  <div class="callout-actions">
    <a class="button primary" href="mailto:hani@vt.edu">Email Me</a>
    <a class="button ghost" href="https://www.linkedin.com/in/hanialomari">LinkedIn</a>
  </div>
</section>
